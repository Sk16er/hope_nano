{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on the production-grade implementation, featuring the Chunkwise Parallel Scan for efficient training and Stateful Inference for $O(1)$ generation.\n",
        "\n",
        "Reppo --> https://github.com/Sk16er/hope_nano\n",
        "\n",
        "Made by [Shushank](https://shushank.site)"
      ],
      "metadata": {
        "id": "hWkK8-md5Ja-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbeZfHwe4qNv"
      },
      "outputs": [],
      "source": [
        "# @title Installation\n",
        "!pip install -q tiktoken datasets matplotlib tqdm seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports & Setup\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # For memory visualization\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "# Setup device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "wOPbopjb4xth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configuration\n",
        "@dataclass\n",
        "class HOPEConfig:\n",
        "    vocab_size: int = 50257 # GPT-2 vocab size\n",
        "    n_embd: int = 384  # Reduced for Colab demo\n",
        "    n_head: int = 6    # Reduced for Colab demo\n",
        "    n_layer: int = 6    # Reduced for Colab demo\n",
        "    block_size: int = 256\n",
        "    dropout: float = 0.1\n",
        "    bias: bool = False\n",
        "\n",
        "    # HOPE specific\n",
        "    cms_update_periods: List[int] = field(default_factory=lambda: [1, 4, 16])\n",
        "    learning_rate_memory: float = 1e-2\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.n_embd % self.n_head == 0\n",
        "\n",
        "config = HOPEConfig()\n",
        "print(f\"Model size: ~{sum(config.n_layer * config.n_head * (config.n_embd//config.n_head)**2 for _ in range(2)) / 1e6:.1f}M parameters (approx)\")"
      ],
      "metadata": {
        "id": "F5un616Y4yO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CMS Architectural Clarification\n",
        "The Continuum Memory System (CMS) Block is structurally implemented here as a standard Multi-Layer Perceptron (MLP). Conceptually, the CMS is intended to capture longer-term knowledge, often through multi-rate updates (e.g., updating parameters only every $\\small{N}$ steps). While the current implementation does not enforce multi-rate logic, its purpose is to create the hierarchical memory structure of the HOPE architecture."
      ],
      "metadata": {
        "id": "ivoy-rgV5Loe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HOPE Model Components (TitansL2, CMSBlock, HOPE)\n",
        "# Content from model.py\n",
        "class TitansL2(nn.Module):\n",
        "    \"\"\"Titans Memory Module with L2/Delta Rule Update and Chunkwise Parallel Scan.\"\"\"\n",
        "    def __init__(self, config: HOPEConfig):\n",
        "        super().__init__()\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        self.chunk_size = 128\n",
        "\n",
        "        # Projections\n",
        "        self.c_q = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.c_k = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.c_v = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        # Learnable parameters (bounded to prevent explosion)\n",
        "        self.alpha_raw = nn.Parameter(torch.zeros(1, self.n_head, 1, 1))\n",
        "        self.beta_raw = nn.Parameter(torch.zeros(1, self.n_head, 1, 1))\n",
        "\n",
        "    @property\n",
        "    def alpha(self):\n",
        "        return torch.sigmoid(self.alpha_raw) * 0.5\n",
        "\n",
        "    @property\n",
        "    def beta(self):\n",
        "        return torch.sigmoid(self.beta_raw) * 0.5\n",
        "\n",
        "    def forward(self, x: torch.Tensor, state: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        B, T, C = x.size()\n",
        "        q = self.c_q(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = self.c_k(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = self.c_v(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = F.normalize(k, dim=-1)\n",
        "\n",
        "        if state is not None:\n",
        "            # Inference Mode: single step update\n",
        "            return self._forward_recurrent(q, k, v, state)\n",
        "        else:\n",
        "            # Training Mode: Chunkwise Parallel Scan (for efficiency)\n",
        "            return self._forward_parallel(q, k, v)\n",
        "\n",
        "    def _forward_recurrent(self, q, k, v, state):\n",
        "        # q, k, v: (B, H, 1, D)\n",
        "        # state: (B, H, D, D)\n",
        "\n",
        "        # 1. Read: y = q @ M^T\n",
        "        y = torch.matmul(q, state.transpose(-1, -2))\n",
        "\n",
        "        # 2. Update\n",
        "        k_t = k.transpose(-1, -2) # (B, H, D, 1)\n",
        "        v_t = v.transpose(-1, -2) # (B, H, D, 1)\n",
        "\n",
        "        # M_new = M - alpha * (M k) k^T + beta * v k^T\n",
        "        Mk = torch.matmul(state, k_t)\n",
        "        forget_term = torch.matmul(Mk, k)\n",
        "        write_term = torch.matmul(v_t, k)\n",
        "\n",
        "        new_state = state - self.alpha * forget_term + self.beta * write_term\n",
        "\n",
        "        B, H, T, D = y.shape\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, self.n_embd)\n",
        "\n",
        "        return self.c_proj(y), new_state\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def _forward_parallel(self, q, k, v):\n",
        "        \"\"\"Sequential loop for demo simplicity (using chunking from model.py)\"\"\"\n",
        "        B, H, T, D = q.shape\n",
        "        # NOTE: Using the simple sequential loop from the original Colab for training clarity\n",
        "        # The full Chunkwise Parallel Scan from model.py is computationally complex for a demo\n",
        "        M = torch.zeros(B, H, D, D, device=q.device, dtype=q.dtype)\n",
        "        ys = []\n",
        "\n",
        "        # Fallback to simple sequential loop for demo clarity (less efficient than full parallel scan)\n",
        "        for t in range(T):\n",
        "            q_t = q[:, :, t:t+1, :]\n",
        "            k_t = k[:, :, t:t+1, :]\n",
        "            v_t = v[:, :, t:t+1, :]\n",
        "\n",
        "            y_t = torch.matmul(q_t, M.transpose(-1, -2))\n",
        "            ys.append(y_t)\n",
        "\n",
        "            k_col = k_t.transpose(-1, -2)\n",
        "            v_col = v_t.transpose(-1, -2)\n",
        "            Mk = torch.matmul(M, k_col)\n",
        "            M = M - self.alpha * torch.matmul(Mk, k_t) + self.beta * torch.matmul(v_col, k_t)\n",
        "\n",
        "        y = torch.cat(ys, dim=2).transpose(1, 2).contiguous().view(B, T, self.n_embd)\n",
        "        return self.c_proj(y), M\n",
        "\n",
        "class CMSBlock(nn.Module):\n",
        "    \"\"\"Continuum Memory System Block (Standard MLP).\"\"\"\n",
        "    def __init__(self, config: HOPEConfig):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class HOPEBlock(nn.Module):\n",
        "    def __init__(self, config: HOPEConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.titans = TitansL2(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.cms = CMSBlock(config)\n",
        "\n",
        "    def forward(self, x, state: Optional[torch.Tensor] = None):\n",
        "        res, new_state = self.titans(self.ln1(x), state)\n",
        "        x = x + res\n",
        "        x = x + self.cms(self.ln2(x))\n",
        "        return x, new_state\n",
        "\n",
        "class HOPE(nn.Module):\n",
        "    def __init__(self, config: HOPEConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "        self.blocks = nn.ModuleList([HOPEBlock(config, i) for i in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None, states=None, pos_offset=0):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "\n",
        "        # CRITICAL FIX: Use pos_offset for stateful generation\n",
        "        pos = torch.arange(pos_offset, pos_offset + t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.wte(idx)\n",
        "        pos_emb = self.wpe(pos % self.config.block_size) # Use % block_size for safety\n",
        "        x = self.drop(tok_emb + pos_emb)\n",
        "\n",
        "        new_states = []\n",
        "\n",
        "        states = states if states is not None else [None] * self.config.n_layer\n",
        "\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, new_block_state = block(x, state=states[i])\n",
        "            new_states.append(new_block_state)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss, new_states\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"Stateful generation with correct positional encoding.\"\"\"\n",
        "        # 1. Prefill\n",
        "        logits, _, states = self(idx, pos_offset=0)\n",
        "\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        out = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        # 2. Generation Loop: O(1) per token\n",
        "        current_pos = idx.size(1)\n",
        "\n",
        "        for _ in range(max_new_tokens - 1):\n",
        "            logits, _, states = self(idx_next, states=states, pos_offset=current_pos)\n",
        "\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            out = torch.cat((out, idx_next), dim=1)\n",
        "            current_pos += 1\n",
        "\n",
        "        return out\n",
        "\n",
        "model = HOPE(config).to(device)\n",
        "\n",
        "if hasattr(torch, 'compile'):\n",
        "    print(\"Compiling model with torch.compile (6-10x speedup)...\")\n",
        "    model = torch.compile(model)\n",
        "    print(\"✓ Compilation enabled\")\n",
        "else:\n",
        "    print(\"⚠ torch.compile not available (update PyTorch to 2.0+)\")\n",
        "\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
      ],
      "metadata": {
        "id": "K7kCM1LM490K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traning\n",
        "This section sets up the data pipeline and runs the stateful training loop, including loss tracking and periodic state resets."
      ],
      "metadata": {
        "id": "1zCck54U5dmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Streaming Dataset (Memory Efficient)\n",
        "class StreamingTextDataset(IterableDataset):\n",
        "    \"\"\"Memory-efficient streaming dataset\"\"\"\n",
        "    def __init__(self, split=\"train\", block_size=config.block_size):\n",
        "        self.dataset = load_dataset(\"roneneldan/TinyStories\", split=split, streaming=True)\n",
        "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        buffer = []\n",
        "        for item in self.dataset:\n",
        "            tokens = self.tokenizer.encode(item['text'])\n",
        "            buffer.extend(tokens)\n",
        "            while len(buffer) >= self.block_size + 1:\n",
        "                chunk = buffer[:self.block_size + 1]\n",
        "                buffer = buffer[self.block_size:]\n",
        "                x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "                y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "                yield x, y\n",
        "\n",
        "train_dataset = StreamingTextDataset(split=\"train\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=8)\n",
        "print(\"✓ Streaming dataset ready\")"
      ],
      "metadata": {
        "id": "tJqkdodQ5AdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Loop (STATEFUL + AMP)\n",
        "# Hyperparameters\n",
        "max_iters = 5000\n",
        "learning_rate = 3e-4\n",
        "min_lr = 3e-5\n",
        "warmup_iters = 200\n",
        "grad_clip = 1.0\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
        "scaler = GradScaler()\n",
        "\n",
        "def get_lr(it):\n",
        "    \"\"\"Cosine learning rate schedule with warmup\"\"\"\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    if it > max_iters:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# CRITICAL FIX: Persistent states across batches\n",
        "persistent_states = None\n",
        "loss_history = []\n",
        "state_reset_interval = 500\n",
        "\n",
        "model.train()\n",
        "train_iter = iter(train_loader)\n",
        "pbar = tqdm(range(max_iters), desc=\"Training\")\n",
        "\n",
        "for step in pbar:\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    try:\n",
        "        X, Y = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        X, Y = next(train_iter)\n",
        "\n",
        "    X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "    # Forward pass with state persistence\n",
        "    with autocast():\n",
        "        logits, loss, new_states = model(X, Y, states=persistent_states)\n",
        "\n",
        "    # Detach states to prevent backprop through time explosion\n",
        "    persistent_states = [s.detach() if s is not None else None for s in new_states]\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "    pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{lr:.2e}'})\n",
        "\n",
        "    # Reset states periodically to prevent drift\n",
        "    if step % state_reset_interval == 0 and step > 0:\n",
        "        persistent_states = None\n",
        "\n",
        "print(\"\\n✓ Training complete!\")\n",
        "\n",
        "# Plotting the Training Loss (New/Improved Visualization)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss (Stateful)\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JH4gbo5x5CGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stateful Generation and Memory Visualization\n",
        "\n",
        "This section demonstrates the $\\small{O(1)}$ stateful generation process and includes the new visualization step to see the Titans memory matrix $\\small{M}$ change in real-time."
      ],
      "metadata": {
        "id": "4KU2Kz5V5toV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Stateful Generation and Memory Visualization\n",
        "@torch.no_grad()\n",
        "def visualize_and_generate(model, prompt, max_tokens=20):\n",
        "    model.eval()\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    tokens = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # 1. Prefill and get initial state\n",
        "    logits, _, states = model(tokens, pos_offset=0)\n",
        "    next_token = torch.multinomial(F.softmax(logits[0, -1] / 0.8, dim=-1), 1)\n",
        "\n",
        "    print(\"=\"*40)\n",
        "    print(\"PROMPT:\", prompt)\n",
        "    print(\"Generated Text (Step-by-step):\")\n",
        "\n",
        "    current_pos = tokens.size(1)\n",
        "    out = tokenizer.decode(tokens[0].tolist())\n",
        "\n",
        "    for i in range(max_tokens):\n",
        "        # 2. Generation Step: O(1) per token\n",
        "        logits, _, states = model(next_token.unsqueeze(0), states=states, pos_offset=current_pos)\n",
        "\n",
        "        # --- Memory Visualization (NEW) ---\n",
        "        if i == 0 or i == max_tokens - 1:\n",
        "            # Visualize the memory state M for the first head of the first layer\n",
        "            M_state = states[0][0, 0].cpu().numpy() # [Layer 0, Head 0, D, D]\n",
        "\n",
        "            plt.figure(figsize=(6, 5))\n",
        "            sns.heatmap(M_state, cmap='viridis', square=True,\n",
        "                        cbar_kws={'label': 'Memory Value'},\n",
        "                        vmax=0.1, vmin=-0.1) # Bounding for clear color contrast\n",
        "            plt.title(f\"Layer 0, Head 0 Memory State (Step {i+1}/{max_tokens})\")\n",
        "            plt.ylabel(\"Value Dim\")\n",
        "            plt.xlabel(\"Key Dim\")\n",
        "            plt.show()\n",
        "\n",
        "        # Sample next token\n",
        "        next_token = torch.multinomial(F.softmax(logits[0, -1] / 0.8, dim=-1), 1)\n",
        "\n",
        "        out += tokenizer.decode(next_token.squeeze().tolist())\n",
        "        print(f\"[{i+1}] {out} | M Updated.\")\n",
        "        current_pos += 1\n",
        "\n",
        "    print(\"=\"*40)\n",
        "    print(\"\\nFINAL OUTPUT:\\n\", out)\n",
        "    return out\n",
        "\n",
        "# Run the demo\n",
        "prompt = \"Once upon a time, a small mouse named Timmy \"\n",
        "visualize_and_generate(model, prompt, max_tokens=5)"
      ],
      "metadata": {
        "id": "L5txHIkl5EI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}